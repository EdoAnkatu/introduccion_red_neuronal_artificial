{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6454adf",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "## 1. Modelo\n",
    "\n",
    "### 1.1. Funciones\n",
    "\n",
    "Una red neuronal artificial está basada en combinaciones lineales de la forma\n",
    "\n",
    "$$ y(\\boldsymbol{x}, \\boldsymbol{w}) = f\\left(\\sum_{j=1}^{M} w_{j}\\phi_{j}\\left(\\boldsymbol{x}\\right)\\right) $$\n",
    "\n",
    "Donde $f(\\cdot)$ es una función no lineal. En la primera capa se realizan las combinaciones lineales de las variables de entrada, donde $w_{j0}$ es el sesgo (_bias_) y $w_{ji}$ es el peso de la variable.\n",
    "\n",
    "$$ a_{j}^{(1)} = \\sum_{i=1}^{D} w_{ji}^{(1)}x_{i} + w_{j0}^{(1)} $$\n",
    "\n",
    "A $a_{j}$ se le llama _activación_. Luego a esta activaciónse le aplica una función de activación que puede ser no lineal.\n",
    "\n",
    "$$ z_{j}^{(L)} = h(a_{j}^{(L)}) $$\n",
    "\n",
    "Luego, para la subsiguiente $L$ capa oculta para el nodo k-ésimo tenemos:\n",
    "\n",
    "$$ a_{k}^{(L+1)} = \\sum_{j=1}^{M} w_{kj}^{(L)}z_{j}^{(L-1)} + w_{k0}^{(L)} $$\n",
    "\n",
    "Esta última ecuación se puede reescribir poniendo una variable _dummy_ $z_{j}^{(L-1)} = 1$ para incorporar el sesgo en la sumatoria.\n",
    "\n",
    "$$ a_{k}^{(L+1)} = \\sum_{j=0}^{M} w_{kj}^{(L)}z_{j}^{(L-1)} $$\n",
    "\n",
    "Para la última capa hay varias opciones dependiendo del problema. Si es regresión simplemente se tiene\n",
    "\n",
    "$$ y_{k} = a_{k} $$\n",
    "\n",
    "Si la variable de salida es binaria se tiene la función sigmoide. Pero si el problema es multiclase entonces se aplica la función _softmax_ que es una generalización de la función sigmoide.\n",
    "\n",
    "$$ y_{k} = \\sigma{\\left(a_{k}\\right)} $$\n",
    "\n",
    "### 1.2. Aprendizaje\n",
    "\n",
    "Considerando un problema de clasificación donde la salida de la red debe ser binaria, es decir hay que clasificar en dos clases $\\mathcal{C}_{1}$ y $\\mathcal{C}_{2}$. Dicho nodo de salida sigue entonces la distribución Bernoulli en una sola realización, de modo que se tiene las probabilidades de cada clase, es decir dada una observación queremos computar $p(\\mathcal{C}_{1}|\\boldsymbol{x})$ o $p(\\mathcal{C}_{2}|\\boldsymbol{x})$. La probabilidad de éxito o fracaso está dado por la función $y(\\boldsymbol{x},\\boldsymbol{w})$. Entonces recordando la función de probabilidad de masa Bernoulli se tiene:\n",
    "\n",
    "$$ p(t|\\boldsymbol{x},\\boldsymbol{w}) = y(\\boldsymbol{x},\\boldsymbol{w})^{t}\\left\\{1-y(\\boldsymbol{x},\\boldsymbol{w})\\right\\}^{1-t}$$\n",
    "\n",
    "Luego, para $N$ ejemplos de entrenamiento con sus vectores $\\boldsymbol{x} = [x_{1},\\dots,x_{N}]^{T}$ y $\\boldsymbol{t} = [t_{1},\\dots,t_{N}]^{T}$ tomamos el logaritmo de la verosimilitud de esta expresión.\n",
    "\n",
    "$$ \\mathcal{L} = -\\ln{\\prod_{n=1}^{N} p(t_{n}|x_{n},\\boldsymbol{w})} $$\n",
    "\n",
    "$$ \\mathcal{L} = -\\ln{\\prod_{n=1}^{N} y(x_{n},\\boldsymbol{w})^{t_{n}}\\left[1-y(x_{n},\\boldsymbol{w})\\right]^{1-t_{n}}} $$\n",
    "\n",
    "$$ \\mathcal{L} = -\\sum_{n=1}^{N} \\ln{\\left\\{y(x_{n},\\boldsymbol{w})^{t_{n}}\\left[1-y(x_{n},\\boldsymbol{w})\\right]^{1-t_{n}} \\right\\}} $$\n",
    "\n",
    "$$ \\mathcal{L} = -\\sum_{n=1}^{N} \\left\\{t_{n}\\ln{y(x_{n},\\boldsymbol{w})}+\\left(1-t_{n}\\right)\\ln{\\left[1-y(x_{n},\\boldsymbol{w})\\right]} \\right\\} $$\n",
    "\n",
    "A esta expresión se le denomina _entropía cruzada_, y es la que definimos como función de error para la red neuronal. Poniendo la notación $y(x_{n},\\boldsymbol{w}) = y_{n}$ entonces la función error a minimizar (equivalente a maximizar la verosimilitud) es:\n",
    "\n",
    "$$ E(\\boldsymbol{w}) = - \\sum_{n=1}^{N} \\left\\{t_{n}\\ln{y_{n}}+(1-t_{n})\\ln{(1-y_{n})} \\right\\} $$\n",
    "\n",
    "Considerando en un problema de clasificación que la función sigmoide está dada por la siguiente relación en un nodo de salida.\n",
    "\n",
    "$$ y = \\sigma{\\left(a\\right)} = \\frac{1}{1+\\exp{(-a)}} $$\n",
    "\n",
    "Para un aprendizaje de los pesos debemos tener el algoritmo de optimización.\n",
    "\n",
    "$$ \\boldsymbol{w}^{(\\tau+1)} = \\boldsymbol{w}^{(\\tau)}-\\eta \\nabla E\\left(\\boldsymbol{w}^{(\\tau)}\\right) $$\n",
    "\n",
    "### 1.3. Retropropagación del error\n",
    "\n",
    "El objetivo es obtener el gradiente de $\\boldsymbol{w}$, lo cual se realiza mediante la regla de la cadena. Se desea obtener el cambio en el error $E$ cuando cambia el peso $w_{ji}$, esto es:\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w_{ji}}} = \\frac{\\partial{E}}{\\partial{a_{j}}} \\frac{\\partial{a_{j}}}{\\partial{w_{ji}}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
